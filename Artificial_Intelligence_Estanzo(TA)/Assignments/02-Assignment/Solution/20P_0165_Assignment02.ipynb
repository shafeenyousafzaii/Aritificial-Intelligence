{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n83Hl4LBy-Mu",
    "outputId": "7c3973bf-6e5f-4749-c614-ceda8cfdfbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "/content/drive/My Drive/Assignment_02\n"
     ]
    }
   ],
   "source": [
    "# Name: Jawad Ahmed\n",
    "# Roll No: 20P-0165\n",
    "# Section: BCS-6A\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# This will create a folder in your drive named 'Colab Notebooks' \n",
    "# I have placed the text files in that folder so if I will use os.getcwd() that path I will get \n",
    "# I have passed that directory to the readfiles function that will read all the txt files or \n",
    "# the file extension you passed in the function parameter.\n",
    "%cd /content/drive/My Drive/Assignment_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwVKtfTXzBma",
    "outputId": "c104ea17-9306-4cad-b50c-262747047193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jawad_ahmed/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jawad_ahmed/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/jawad_ahmed/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Name: Jawad Ahmed\n",
    "# Roll No: 20P-0165\n",
    "# Section: BCS-6A\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class FrequencyAnalyzer:\n",
    "    def __init__(self, directory):\n",
    "        # Constructor to initialize directory and frequency dictionary\n",
    "        self.directory = directory\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Remove leading and trailing white spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        # Remove non-alphabetic characters and convert to lowercase\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # Tokenize text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stop words and lemmatize words\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stopwords]\n",
    "\n",
    "        # Convert list of words back to text\n",
    "        text = ' '.join(words)\n",
    "\n",
    "         # Normalize white space\n",
    "        text = re.sub('\\s+', ' ', text).strip()\n",
    "    \n",
    "        # Handle special characters\n",
    "        text = text.replace('&', ' and ')\n",
    "        text = text.replace('@', ' at ')\n",
    "        text = text.replace('#', ' number ')\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        r_dict = {i: 0 for i in range(32, 127)}\n",
    "        r_dict[10] = 0\n",
    "        for filename in os.listdir(self.directory):\n",
    "            file_path = os.path.join(self.directory, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    text = file.read()\n",
    "#                     preprocessed_text = self.preprocess_text(text)\n",
    "                    preprocess_text = text\n",
    "                    for char in preprocessed_text:\n",
    "                        ascii_code = ord(char)\n",
    "                        if ascii_code >= 32 and ascii_code <= 126:\n",
    "                            r_dict[ascii_code] += 1\n",
    "                        elif ascii_code == 10:\n",
    "                            r_dict[10] += 1\n",
    "        return r_dict\n",
    "\n",
    "\n",
    "    \n",
    "    def generate_cipher(self, input_file, freq_dict):\n",
    "#         Compute the frequencies\n",
    "        r_dict = {i:0 for i in range(32, 127)}\n",
    "        r_dict[10] = 0\n",
    "        with open(input_file, 'r') as file:\n",
    "            for line in file:\n",
    "                for char in line:\n",
    "                    ascii_code = ord(char)\n",
    "                    if (ascii_code >= 32 and ascii_code <= 126):\n",
    "                        r_dict[ascii_code] += 1\n",
    "                    elif ascii_code == 10:\n",
    "                        r_dict[10] += 1\n",
    "        cipher_dict = self.compare_dictionaries(freq_dict, r_dict)\n",
    "        print(cipher_dict)\n",
    "        self.store_cipher_file(cipher_dict)\n",
    "        self.generate_plain_text(cipher_dict, input_file)\n",
    "        # self.apply_word_net()\n",
    "        print(\"Done\")\n",
    "        \n",
    "    \n",
    "    def store_cipher_file(self, dict_one):\n",
    "        with open('generated_cipher.txt', \"w\") as file:\n",
    "            for value in dict_one.values():\n",
    "                file.write(chr(value))\n",
    "#     Agar wo cipher hma values ma mil jata ha \n",
    "#     To osa key sa replace kr do\n",
    "                \n",
    "    def generate_plain_text(self, dict_one, input_file):\n",
    "        with open(input_file, 'r') as input_file:\n",
    "            with open('output.txt', 'w') as output_file:\n",
    "                while True:\n",
    "                    char = input_file.read(1)  # Read one character at a time\n",
    "                    if not char:  # End of file\n",
    "                        break\n",
    "                    code = ord(char)  # Convert character to ASCII code\n",
    "                    if code in dict_one.values():\n",
    "                        # Find the corresponding key for the given value (code)\n",
    "                        key = list(dict_one.keys())[list(dict_one.values()).index(code)]\n",
    "                        output_file.write(chr(key))  # Write the corresponding key as character to output file\n",
    "                    else:\n",
    "                        output_file.write(char) # Write the character as it is if it is not in the frequency dictionary\n",
    "\n",
    "\n",
    "    def apply_word_net(self):\n",
    "        with open('output.txt', 'r') as file:\n",
    "            data = file.read().replace('\\n', ' ')\n",
    "            words = data.split()\n",
    "        for i in range(len(words)):\n",
    "            if not wordnet.synsets(words[i]):\n",
    "                similar_words = []\n",
    "                for syn in wordnet.synsets(words[i]):\n",
    "                    for lemma in syn.lemmas():\n",
    "                        if lemma.name() != words[i]:\n",
    "                            similar_words.append(lemma.name())\n",
    "                if similar_words:\n",
    "                    words[i] = similar_words[0]\n",
    "        corrected_text = ' '.join(words)\n",
    "        with open('output.txt', 'w') as file:\n",
    "            file.write(corrected_text)\n",
    "\n",
    "    \n",
    "    def compare_dictionaries(self, dict_one, dict_two):\n",
    "        sorted_dict_one = sorted(dict_one.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_dict_two = sorted(dict_two.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        dictionary_three = {}\n",
    "        for i in range(len(sorted_dict_one)):\n",
    "            max_key_1 = sorted_dict_one[i][0]\n",
    "            max_value_1 = sorted_dict_one[i][1]\n",
    "            max_key_2 = sorted_dict_two[i][0]\n",
    "            max_value_2 = sorted_dict_two[i][1]\n",
    "\n",
    "            dictionary_three[max_key_1] = max_key_2\n",
    "\n",
    "        return dictionary_three\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-9zimcy623IY"
   },
   "outputs": [],
   "source": [
    "analyzer = FrequencyAnalyzer('model_train_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BwTlEgM827-b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_train_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e0e9d1996dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreq_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-d6bb02af8a13>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mr_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m127\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_train_files'"
     ]
    }
   ],
   "source": [
    "freq_dict = analyzer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4agOjtz2-F9",
    "outputId": "081f6cda-1822-41cb-b127-accd4a83fa7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{32: 65766910, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 33275489, 98: 6291361, 99: 16235947, 100: 18326337, 101: 50100790, 102: 6362795, 103: 10790127, 104: 10925912, 105: 30190900, 106: 1067190, 107: 4442123, 108: 21565303, 109: 12229955, 110: 29710455, 111: 27094020, 112: 10681019, 113: 523119, 114: 30514415, 115: 21936871, 116: 28804299, 117: 13205871, 118: 4954203, 119: 5657670, 120: 1000219, 121: 7316325, 122: 565544, 123: 0, 124: 0, 125: 0, 126: 0, 10: 0}\n"
     ]
    }
   ],
   "source": [
    "print(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpRNPi_03A1h",
    "outputId": "f2ac09bf-bcdf-421c-ace6-cebb5fcc1325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{32: 32, 101: 87, 97: 83, 114: 89, 105: 68, 110: 66, 116: 123, 111: 42, 115: 102, 108: 116, 100: 48, 99: 122, 117: 58, 109: 106, 104: 93, 103: 33, 112: 64, 121: 75, 102: 120, 98: 79, 119: 96, 118: 99, 107: 10, 106: 107, 120: 56, 122: 62, 113: 67, 33: 105, 34: 51, 35: 60, 36: 88, 37: 94, 38: 111, 39: 34, 40: 35, 41: 36, 42: 37, 43: 38, 44: 39, 45: 40, 46: 41, 47: 43, 48: 44, 49: 45, 50: 46, 51: 47, 52: 49, 53: 50, 54: 52, 55: 53, 56: 54, 57: 55, 58: 57, 59: 59, 60: 61, 61: 63, 62: 65, 63: 69, 64: 70, 65: 71, 66: 72, 67: 73, 68: 74, 69: 76, 70: 77, 71: 78, 72: 80, 73: 81, 74: 82, 75: 84, 76: 85, 77: 86, 78: 90, 79: 91, 80: 92, 81: 95, 82: 97, 83: 98, 84: 100, 85: 101, 86: 103, 87: 104, 88: 108, 89: 109, 90: 110, 91: 112, 92: 113, 93: 114, 94: 115, 95: 117, 96: 118, 123: 119, 124: 121, 125: 124, 126: 125, 10: 126}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "analyzer.generate_cipher('Files/subs/encrypted1.txt', freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_rfR-SN481S",
    "outputId": "a0ebdbc7-547c-4dd0-bfc0-7a8a7e19a119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geadeei are tlecnpil are uecmtabngeadeei are wsatsinpil are chanycmmo are jrclsdnysu frtie to are qtiklswn   ngeadeei are hsihevatsinpil are huecatsingeadeei are ewsatsinpil are ueovsioenycmmo are jrclsdn!txe to &eub msikn   ngeadeei are leotuenpil are ovcowngeadeei are vsaeihbnpil are e%toaeihengeadeei are eooeihenpil are leoheianycmmo are jrclsdnysu frtie to are qtiklswn   nysu frtie ton!txe tonysu frtie to aren   nfrto to are dcb are dsuml eilonfrto to are dcb are dsuml eilonfrto to are dcb are dsuml eilon$sa dtar c zcik z\"a c drtwveu#n\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'r') as file:\n",
    "  print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLVzDczN5Lxg",
    "outputId": "520f9134-0859-4d0f-a9f8-775c9b5eefcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jMd-gtIh5hvK"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(expected_output_path, predicted_output_path):\n",
    "    # read the contents of both files\n",
    "    with open(expected_output_path, 'r') as expected_file:\n",
    "        expected_text = expected_file.read()\n",
    "    with open(predicted_output_path, 'r') as predicted_file:\n",
    "        predicted_text = predicted_file.read()\n",
    "\n",
    "    # remove whitespace and newlines from both files\n",
    "    expected_text = expected_text.replace(' ', '').replace('\\n', '')\n",
    "    predicted_text = predicted_text.replace(' ', '').replace('\\n', '')\n",
    "\n",
    "    # calculate the number of matching characters\n",
    "    num_correct = sum([1 for i in range(len(expected_text)) if expected_text[i] == predicted_text[i]])\n",
    "\n",
    "    # calculate the accuracy as a percentage\n",
    "    accuracy = round(num_correct / len(expected_text) * 100, 2)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ekonGZL9jFm",
    "outputId": "624cf24b-8953-4c58-9d3a-ab6c05518b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 6.79%\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy('Files/subs/plain1.txt', 'output.txt')\n",
    "print(f'The accuracy of the model is {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8F6dtar9thc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
